{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('base': conda)",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c8c3a880c14b61cd710572e2ac6298335c33509b1f7d3900bc107f98e9cc647"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Feature selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When you are done creating hundreds of thousands of features, it's time for selecting a few of them . for training the model. Having too many features pose a problem well known as the curse of dimensionality. If you have a lot of features , you must also have alot of training samples to capture all the features. what's considered a \"lot\" is not defined correctly and it is upto us to figure out by validating our models properly and checking how much time it takes to train your models.\n",
    "\n",
    "The simplest form of selecting features would be to **remove features with very low variance.** If the features have a low variance (i.e, very close to 0), they are close to being constant and thus , do not add any values to any model at all. It would be nice to get rid of them and hence lower the complexity. \n",
    "\n",
    "Also note that the variance also depends on scaling of the data. Scikit-learn has an implementation for **VarianceThreshold** that does precisely this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "data = .... \n",
    "var_thresh = VarianceThreshold(threshold= 0.1)\n",
    "transformed_data = var_thresh.fit_transform(data)\n",
    "\n",
    "# Transformed data will have all columns with variance less than 0.1 removed"
   ]
  },
  {
   "source": [
    "we can also removed features which have a high correlation. for calculating the correlation between different numerical features, we can use the **Pearson correlation**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\nMedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \nHouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \nAveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \nAveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \nPopulation   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \nAveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \nLatitude    -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \nLongitude   -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \nMedInc_Sqrt  0.984329 -0.132797  0.326688  -0.066910    0.018415  0.015266   \n\n             Latitude  Longitude  MedInc_Sqrt  \nMedInc      -0.079809  -0.015176     0.984329  \nHouseAge     0.011173  -0.108197    -0.132797  \nAveRooms     0.106389  -0.027540     0.326688  \nAveBedrms    0.069721   0.013344    -0.066910  \nPopulation  -0.108785   0.099773     0.018415  \nAveOccup     0.002366   0.002476     0.015266  \nLatitude     1.000000  -0.924664    -0.084303  \nLongitude   -0.924664   1.000000    -0.015569  \nMedInc_Sqrt -0.084303  -0.015569     1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>MedInc_Sqrt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MedInc</th>\n      <td>1.000000</td>\n      <td>-0.119034</td>\n      <td>0.326895</td>\n      <td>-0.062040</td>\n      <td>0.004834</td>\n      <td>0.018766</td>\n      <td>-0.079809</td>\n      <td>-0.015176</td>\n      <td>0.984329</td>\n    </tr>\n    <tr>\n      <th>HouseAge</th>\n      <td>-0.119034</td>\n      <td>1.000000</td>\n      <td>-0.153277</td>\n      <td>-0.077747</td>\n      <td>-0.296244</td>\n      <td>0.013191</td>\n      <td>0.011173</td>\n      <td>-0.108197</td>\n      <td>-0.132797</td>\n    </tr>\n    <tr>\n      <th>AveRooms</th>\n      <td>0.326895</td>\n      <td>-0.153277</td>\n      <td>1.000000</td>\n      <td>0.847621</td>\n      <td>-0.072213</td>\n      <td>-0.004852</td>\n      <td>0.106389</td>\n      <td>-0.027540</td>\n      <td>0.326688</td>\n    </tr>\n    <tr>\n      <th>AveBedrms</th>\n      <td>-0.062040</td>\n      <td>-0.077747</td>\n      <td>0.847621</td>\n      <td>1.000000</td>\n      <td>-0.066197</td>\n      <td>-0.006181</td>\n      <td>0.069721</td>\n      <td>0.013344</td>\n      <td>-0.066910</td>\n    </tr>\n    <tr>\n      <th>Population</th>\n      <td>0.004834</td>\n      <td>-0.296244</td>\n      <td>-0.072213</td>\n      <td>-0.066197</td>\n      <td>1.000000</td>\n      <td>0.069863</td>\n      <td>-0.108785</td>\n      <td>0.099773</td>\n      <td>0.018415</td>\n    </tr>\n    <tr>\n      <th>AveOccup</th>\n      <td>0.018766</td>\n      <td>0.013191</td>\n      <td>-0.004852</td>\n      <td>-0.006181</td>\n      <td>0.069863</td>\n      <td>1.000000</td>\n      <td>0.002366</td>\n      <td>0.002476</td>\n      <td>0.015266</td>\n    </tr>\n    <tr>\n      <th>Latitude</th>\n      <td>-0.079809</td>\n      <td>0.011173</td>\n      <td>0.106389</td>\n      <td>0.069721</td>\n      <td>-0.108785</td>\n      <td>0.002366</td>\n      <td>1.000000</td>\n      <td>-0.924664</td>\n      <td>-0.084303</td>\n    </tr>\n    <tr>\n      <th>Longitude</th>\n      <td>-0.015176</td>\n      <td>-0.108197</td>\n      <td>-0.027540</td>\n      <td>0.013344</td>\n      <td>0.099773</td>\n      <td>0.002476</td>\n      <td>-0.924664</td>\n      <td>1.000000</td>\n      <td>-0.015569</td>\n    </tr>\n    <tr>\n      <th>MedInc_Sqrt</th>\n      <td>0.984329</td>\n      <td>-0.132797</td>\n      <td>0.326688</td>\n      <td>-0.066910</td>\n      <td>0.018415</td>\n      <td>0.015266</td>\n      <td>-0.084303</td>\n      <td>-0.015569</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np \n",
    "\n",
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "x = data['data']\n",
    "col_names = data['feature_names']\n",
    "y = data['target']\n",
    "\n",
    "# convert to pandas dataframe \n",
    "df = pd.DataFrame(x , columns=col_names)\n",
    "\n",
    "# introduce a highly correlated column\n",
    "df.loc[: , 'MedInc_Sqrt'] = df.MedInc.apply(np.sqrt)\n",
    "\n",
    "# get correlation matrix (pearson)\n",
    "df.corr()"
   ]
  },
  {
   "source": [
    "we see that the feature **\"MedInc_Sqrt\"** has a very high correlation with **\"MedInc\"**. we can remove one of them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "And now we can move to some univariate ways of feature selection. **univariate feature selection** is nothing but a scoring of each feautre against a given target.\n",
    "\n",
    "**Mutual information, ANOVA F-test** and **$chi^2$** are some of the most popular methods for univariate feature selection. There are two ways of using these in scikit-learn.\n",
    "\n",
    "- **SelectKBest**: It keeps the top-k scoring features\n",
    "- **SelectPrecentile**: It keeps the top features which are in a percentage specified by the user\n",
    "\n",
    "It must b noted that you can use $chi^2$ only for data which is non-negative in nature. This is a particularly useful feature selection technique in natural language processing when we have a bag of words or tf-idf based features. It's best to create a wrapper for univariate feature selection that you can use for almost any new problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " import UnivariateFeatureSelection as ufs \n",
    " \n",
    " # initializing the instance\n",
    " uni_sel = ufs.UnivariateFeatureSelection(\n",
    "     n_features = 0.9,\n",
    "     problem_type = 'regression',\n",
    "     scoring = 'f_regression'\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SelectPercentile(percentile=90,\n                 score_func=&lt;function f_regression at 0x00000178F6D99438&gt;)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "uni_sel.fit(x ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transformed = uni_sel.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(20640, 7)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x_transformed.shape"
   ]
  },
  {
   "source": [
    "Most of the time , people prefer doing feature selection using a machine learning model. let's see how that is done\n",
    "The simplest form of feature selection that uses a model for selection is known as **greedy feature selection**. In greedy feature selection. \n",
    "\n",
    "The first step is to choose a model. The second step is to select a loss/scoring function. And the third and final step is to iteratively evaluate each feature and add it to the list of **\"good\"** features if it improves loss/score. \n",
    "\n",
    "But you must keep this inmind that this is known as greedy feature selection for a reason. This feature selection process will fit a given model each time it evaluates a feature. The computational cost associate with this kind of method is very high. It will also take a lot of time for this kind of feature selection to finish. and if you do not use this feature selection properly . then you might even end up overfitting the model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}